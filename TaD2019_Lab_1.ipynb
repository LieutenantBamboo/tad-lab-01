{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TaD2019 Lab 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LieutenantBamboo/tad-lab-01/blob/master/TaD2019_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1suGewMzIKsC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 1\n",
        "\n",
        "The aims of the lab are to:\n",
        "*   Introduce you to colab, verify that you're setup with the correct python packages\n",
        "*   Introduce you to NLTK and Pandas libraries\n",
        "*   Learn to perform text processing using NLTK: tokenization, normalization, and segmentation of text \n",
        "*   Calculate basic collection statistics of a corpus of text\n",
        "*   Become familiar with the structure of the Reddit data collection\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7656mUzX1dVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Colab Introduction\n",
        "\n",
        "\n",
        "Colab is an improved(?) Jupyter Notebook.  It is used internally by engineers and researchers at Google to prototype and share experimental results. \n",
        "\n",
        "It supports:\n",
        "\n",
        "1. Text Cells with [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) formatting\n",
        "2. Code Cells\n",
        "3. Notebook stores code, output, and execution order\n",
        "4. Tab and Tab + Tab Autocomplete\n",
        "5. IPython Help Features\n",
        "6. IPython Magics (`%%`)\n",
        "\n",
        "### Additional Features\n",
        "\n",
        "- collaborative editing\n",
        "- history \n",
        "- comments\n",
        "- executed code history\n",
        "- Shift+click multiple cell selection\n",
        "- searchable code snipetts + table of contents\n",
        "- scratchpad (⌘/Ctrl + Alt + N)\n",
        "\n",
        "### Keyboard Shortcuts\n",
        "| Command | Action |\n",
        "| ---- | ----: |\n",
        "|⌘/Ctrl+Enter | Run Selected Cell |\n",
        "|Shift+Enter| Run Cell and Select Next |\n",
        "|Alt+Enter| Run cell and insert new cell|\n",
        "|⌘/Ctrl+M I | Interrupt Execution |\n",
        "\n",
        "- You can open the command Palette to see all shortcuts by going to Tools --> Command palette.\n",
        "\n",
        "### Summary of tips\n",
        "- Use TAB to autocomplete an expression. \n",
        "- You can also execute the code with a ? to get the doc strings\n",
        "- In Jupyter / Colab you can execute shell commands using `!`, example: \"!ls\" to list the current files.\n",
        "\n",
        "\n",
        "*Note:* Occassionally colab may hang or crash (due to cloud flakiness or (sometimes) bad code).  You can control the execution using the Runtime menu to reboot and start fresh.  To resume where you left off you can click \"Run before\" and it will run all cells before the one currently selected.\n",
        "\n",
        "\n",
        "*Note:* You can use Colab with a cloud VM or connect to a local Jupyter instance.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1-Xg8pM7q1_E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Getting help example\n",
        "\n",
        "Let's get some help with the Pandas library we'll be using later.  The DataFrame is the main data collection representation in Pandas, so it's important to learn what's possible to do with it. "
      ]
    },
    {
      "metadata": {
        "id": "ifQeh56Kq0SM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Show the method signature and documentation\n",
        "pd.DataFrame?\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fS4Cnp1AdCa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show method source\n",
        "??pd.DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4wCSSml2q5Mz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Executing local commands\n",
        "We can look at the local VM environment properties available in CoLab VM (or local environment).  \n",
        "\n",
        "#### Your task : \n",
        "In the code cell below, create shell commands (one per line) to Inspect the linux version (uname), current directory (pwd), the amount of space (df), and amount of disk (vmstat -s). The output will be printed in the notebook."
      ]
    },
    {
      "metadata": {
        "id": "tYxbwlfA4mWm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rd2TwN0u2vWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup \n",
        "\n",
        "#### Your task:####\n",
        "Just run the cells below, and verify that the output is as expected. If anything looks wrong, weird, or crashes, contact the course staff. We don't want library issues to get in the way of the real work! "
      ]
    },
    {
      "metadata": {
        "id": "LHP4WSZGIKsD",
        "colab_type": "code",
        "outputId": "5cd74d94-451c-44c9-d4d6-393c075f03d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# Version checks\n",
        "import importlib\n",
        "def version_check(libname, min_version):\n",
        "    m = importlib.import_module(libname)\n",
        "    print (\"%s version %s is\" % (libname, m.__version__))\n",
        "    print (\"OK\" if m.__version__ >= min_version \n",
        "           else \"out-of-date. Please upgrade!\")\n",
        "    \n",
        "version_check(\"numpy\", \"1.14\")\n",
        "version_check(\"matplotlib\", \"1.6\")\n",
        "version_check(\"pandas\", \"0.22\")\n",
        "version_check(\"nltk\", \"3.2\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.7 (default, Oct 22 2018, 11:32:17) \n",
            "[GCC 8.2.0]\n",
            "numpy version 1.14.6 is\n",
            "OK\n",
            "matplotlib version 2.1.2 is\n",
            "OK\n",
            "pandas version 0.22.0 is\n",
            "OK\n",
            "nltk version 3.2.5 is\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HOBISPPkBfUY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and process Reddit data\n",
        "In this part of the lab we'll introduce the Reddit data. You will learn how to load and operate on it using the Pandas data library. "
      ]
    },
    {
      "metadata": {
        "id": "napJywoXLO7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reddit Data\n",
        "\n",
        "Social media analysis is an important application of text processing.  Throughout the course we'll be using data from the popular social media website Reddit. The full contents of Reddit is freely available ([full dump](https://files.pushshift.io/reddit/comments/)). For this course we'll be using a cleaned and annotated subset of the data. Note that although it has been 'cleaned', it is still 'raw' forum data. \n",
        "\n",
        "For convenience, we've stored as [JSON](https://www.json.org/) objects. A description of the data fields is below. \n",
        "\n",
        "**Terminology**\n",
        "\n",
        "The main unit of processing is a reddit *thread*; it represents a discussion topic with a unique URL. The thread contains metadata as well as the *posts* within it. A *post* is a single user entry in a thread. A post has the *body*, its *author*, it's position in the thread, as well as other metadata. \n",
        "\n",
        "\n",
        "**Thread fields**\n",
        "*   url - reddit URL of the thread\n",
        "*   title - title of the thread, as written by the first poster\n",
        "*   is_self_post - True if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link)\n",
        "*   subreddit - the subreddit of the thread\n",
        "*   posts - a list of all posts in the thread\n",
        "\n",
        "**Post fields**\n",
        "*   id - post ID, reddit ID of the current post\n",
        "*   body - the body of the post (text + link markup)\n",
        "*   in_reply_to - parent ID, reddit ID of the parent post, or the post that the current post is in reply to\n",
        "*   post_depth - the number of replies the current post is from the initial post\n",
        "*   is_first_post - True if the current post is the initial post\n",
        "\n",
        "Let's download the data.  It should only take a fraction of a second since we're transferring data in the Google cloud. Downloading it locally will take longer.\n"
      ]
    },
    {
      "metadata": {
        "id": "OyYm3gDwJKQ-",
        "colab_type": "code",
        "outputId": "0a360b22-6ed7-496b-b848-512d76912f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# The name of the local file.\n",
        "local_file = \"coarse_discourse_dump_reddit.json\"\n",
        "\n",
        "\n",
        "# Below uses the Google cloud API utilities to copy the file. \n",
        "# This library is available in colab, but is not installed on lab machines.\n",
        "!gsutil cp gs://textasdata/coarse_discourse_dump_reddit.json $local_file\n",
        "  \n",
        "# An alternative curl command when running locally as a backup\n",
        "# !curl -o $local_file https://storage.googleapis.com/tad2018/coarse_discourse_dump_reddit.json\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://textasdata/coarse_discourse_dump_reddit.json...\n",
            "\\ [1 files][ 78.5 MiB/ 78.5 MiB]                                                \n",
            "Operation completed over 1 objects/78.5 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDOn1-DsMC2O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can directly read the Reddit JSON data into a pandas DataFrame.\n",
        "\n",
        "Let's print out a sample of the first few reddit threads using the head() function.\n"
      ]
    },
    {
      "metadata": {
        "id": "qS6FVs_cImTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threads = pd.read_json(path_or_buf=local_file, lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ph3_qktAqcvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task: \n",
        "Use head() to print out the top of the threads DataFrame.  Note: in CoLab the result of the last line in a code block is automatically printed."
      ]
    },
    {
      "metadata": {
        "id": "sFZ5-oRBVLpb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IieBIuj_R1A8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should see the first five threads with the fields discussed above. "
      ]
    },
    {
      "metadata": {
        "id": "MYlNgsos4xTa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## An aside on Pandas\n",
        "\n",
        "Pandas is a widely used Python-standard library for dealing with data. It processes data in a DataFrame, which you can think of as a relational table with columns. Our focus in TaD is not on Pandas or relational data. However, Pandas provides a convenient framework for processing and manipulating all kinds of data (including text). We will use it to store and operate on the Reddit data.\n"
      ]
    },
    {
      "metadata": {
        "id": "DRAlfeFQgrN8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Iteration\n",
        "\n",
        "To iterate through DataFrame's row in pandas one can use:\n",
        "\n",
        "- DataFrame.iterrows() or DataFrame.itertuples()\n",
        "\n",
        "*Question* Which is faster? \n",
        " - We will time them using timeit\n",
        " - We will demonstrate how to use iteration with a \"for comprehension\".  If you don't remember for comprehensions, we suggest you review the Python tutorial linked in Moodle. \n",
        " \n",
        "#### Your Task\n",
        "Execute the code below and read it to understand how it works. In particular, notice the selection of fields from the DataFrame.\n"
      ]
    },
    {
      "metadata": {
        "id": "svp1GxJkVPAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note the use of itertools which has many useful iteration utilities.\n",
        "from itertools import islice\n",
        "\n",
        "N = 5\n",
        "\n",
        "# Print the first N rows using both methods. \n",
        "\n",
        "# Iterrows\n",
        "for index, thread in islice(threads.iterrows(), N) :\n",
        "    print(thread[\"url\"], thread[\"subreddit\"])\n",
        "\n",
        "# Itertuples\n",
        "print (\"Now with itertuples.\")\n",
        "for thread in islice(threads.itertuples(index=True, name='Pandas'), N):\n",
        "    print(getattr(thread, \"url\"), getattr(thread, \"subreddit\"))\n",
        "\n",
        "N = 1000 # We need a bigger N to show iteration time difference.\n",
        "# Iterrows\n",
        "time1 = %timeit [thread[\"url\"] for index, thread in islice(threads.iterrows(), N)]\n",
        "\n",
        "# You could do the same with \n",
        "time2 = %timeit [getattr(thread, \"url\") for thread in islice(threads.itertuples(index=True, name='Pandas'), N)]    \n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xkP8kIymlsA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should find that itertuples is roughly 20x faster; it just creates a tuple and  rather than a full Pandas Series object (see below). "
      ]
    },
    {
      "metadata": {
        "id": "lraRbpSaHiew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On your own you may consider skimming the documentation on [Pandas datastructures](https://pandas.pydata.org/pandas-docs/stable/dsintro.html). \n",
        "\n",
        "\n",
        "We can index into a DataFrame, by asking for particular rows:\n",
        "\n",
        "\n",
        "*   `iloc` allows us to ask for particular row(s) indexed by (integer) position\n",
        "*   `loc` allows us to ask for particular row(s) indexed by a label.\n",
        "\n",
        "By default, the labels are automatically assigned, starting from 0, so the two statements are identical for us.\n",
        "\n",
        "Each row of a DataFrame is a [Series](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#series), a one dimensional labelled array holding any data type.\n"
      ]
    },
    {
      "metadata": {
        "id": "13u5yKflTmha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Task:** Execute the code below to see an example of this in action. "
      ]
    },
    {
      "metadata": {
        "id": "-zl_Lv4DHq3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# See the type of pandas threads object\n",
        "print(type(threads))\n",
        "\n",
        "print(threads.iloc[0])\n",
        "\n",
        "print(threads.loc[1])\n",
        "\n",
        "# See the type of pandas for a single thread (a row)\n",
        "type(threads.iloc[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NaoBpDgEJKxm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use Pandas to print basic statistics on the threads that are built-in (count, describe, etc..)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "C8AQ-F1_UVR6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Collection statistics"
      ]
    },
    {
      "metadata": {
        "id": "zw-7N0YArqjc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's learn more about the Reddit collection statistics. \n",
        "\n",
        "#### Your Task\n",
        "Use count() on the threads object to print a count distribution. \n",
        "- Note: count() gives count values for each column in the frame independently."
      ]
    },
    {
      "metadata": {
        "id": "FoUXuFxiJbYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLdhtT4FZyuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We should have 9401 threads. \n",
        "- Do all columns have the same count? What does this tell us about the collection? \n",
        "\n",
        "Now, let's dig in and explores more statistics on the threads by their subreddit.\n",
        "\n",
        "First, let's select all of the values of the *subreddit* column and inspect it's type.\n"
      ]
    },
    {
      "metadata": {
        "id": "3SE8n0CIItbC",
        "colab_type": "code",
        "outputId": "079f3132-5d8e-4052-db00-81aafe5453c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Select a column of the dataframe\n",
        "subreddits = threads['subreddit']\n",
        "print(type(subreddits))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKNRjOR9sQ97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's a Series, a row in a DataFrame. We will use a useful function on the Series object, [value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) to group and count the values for the *subreddit* series.\n",
        "\n",
        "#### Your task:\n",
        "Use value_counts on the *subbreddits* variable.\n",
        " - Print a statistical summary of the data using .describe() \n",
        " - Use head() to print out the top 5 subreddits with their counts"
      ]
    },
    {
      "metadata": {
        "id": "Lvnaci6YVD1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wEKElyq4RG3W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- What information is provided by the describe() function? \n",
        "- What does this statistical summary tell you about the frequency distrubution of threads in subreddits?  Consider what was discussed in lecture 2 about word distributions; this distribution follows a similar pattern. This is typical of real-world text data and will have important ramifications later in the course."
      ]
    },
    {
      "metadata": {
        "id": "9Qz0aYNbaFeC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also create a simple bar graph to visualize the most popular sub-reddits in the data. \n",
        "\n",
        "The code below creates a simple bar graph of the top 20 sub-reddits by thread frequency."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rSdEA210Vgge",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note: need this for Jupyter on local machines to work\n",
        "%matplotlib inline\n",
        "\n",
        "top_subbreddits = subreddit_counts.nlargest(20).plot.bar()\n",
        "print(top_subbreddits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0kDjxHcGtt3a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reddit data by posts"
      ]
    },
    {
      "metadata": {
        "id": "TTdQPAttaRMK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Above we looked at statistics at the level of whole threads. \n",
        "\n",
        "Threads are meaningful units, but processing them can be unwieldy.  In many cases it's easier to operate at the post level because these contain the text. They are also shorter more manageable chunks of text. To do this, we will flatten the threads to make each post it's own row in the DataFrame. Below is some code the reads in the JSON data into a list and creates a frame where each row is a post (with their source thread metadata). "
      ]
    },
    {
      "metadata": {
        "id": "9mEQjNIGKjZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The reddit thread structure is nested in the posts in a new content.\n",
        "# Read the file as json and create a new posts dataframe.\n",
        "import json\n",
        "\n",
        "# We will append to a list that we will convert to a DataFrame.\n",
        "posts = list()\n",
        "\n",
        "with open(local_file) as jsonfile:\n",
        "  for i, line in enumerate(jsonfile):\n",
        "    thread = json.loads(line)\n",
        "    # Keep information about the source thread where the post comes from.\n",
        "    for post in thread['posts']:\n",
        "      posts.append((thread['subreddit'], thread['title'], post['id'], \n",
        "                    thread['url'], post.get('author', \"\"), \n",
        "                    post.get('body', \"\")))\n",
        "print(len(posts))\n",
        "\n",
        "# A post data as a DataFrame, we could also keep it as a list of tuples.\n",
        "labels = ['subreddit', 'title', 'id', 'url', 'author', 'body']\n",
        "post_frame = pd.DataFrame(posts, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58FUFdnIWL8w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The output is the number of posts.  It should be about 110k posts."
      ]
    },
    {
      "metadata": {
        "id": "pcJw6uCEazgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task\n",
        "Print out the *head* of the posts frame object.  Inspect the output to see what's in the frame and make sure it seems reasonable to you."
      ]
    },
    {
      "metadata": {
        "id": "vWnd9X8aWYy0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RTjDwSXcbIfb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task\n",
        "Let's examine the question of how posts relate to threads. Each post has the thread it came from, defined by its URL.  Select the URLs and count the values.  Print out the statistical summary using describe.  *Hint:* We did something similar for the sub-reddits. "
      ]
    },
    {
      "metadata": {
        "id": "VNghQt_0Xef8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uno8qoyDXEHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Critically look at these statistics.  \n",
        "- What is the shortest thread, longest thread?\n",
        "- How many posts are in a 'typical' thread? \n",
        "- Do they seem reasonable? What does it tell you about threads?  "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rMsrN746IJtr"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task \n",
        "- Create a bar plot of the top 10 authors of reddit posts.\n",
        "\n",
        "*Note:* Not all posts have authors. As a first step you need to `replace` all empty values in the frame with a numpy nan value `np.nan`, after doing this pandas will filter them out automatically."
      ]
    },
    {
      "metadata": {
        "id": "0QQV6brnwxiO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVNXNpRaXlAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CEIvkIJpI4sn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text processing of reddit posts"
      ]
    },
    {
      "metadata": {
        "id": "B6Turx6-IdLD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now take our first steps processing the reddit posts as text.  We will apply an NLTK text processing pipeline to the posts. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NcVR1_CLIKsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NLTK\n",
        "\n",
        "[NLTK](http://www.nltk.org/) is a large compilation of Python NLP packages. It includes implementations of a number of classic NLP models, as well as utilities for working with linguistic data structures, preprocessing text, and managing corpora. NLTK is included with Anaconda.  "
      ]
    },
    {
      "metadata": {
        "id": "aK25xu89Uu3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "NLTK is just one many text processing libraries. Later in the course we will try out [spaCy](https://spacy.io/).\n",
        "\n",
        "First, let's import the library."
      ]
    },
    {
      "metadata": {
        "id": "v5b9IoSdPZRg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gplq-_aryY5y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 1: Tokenization\n",
        "We will try tokenization ourselves using NLTK's tokenizers. You may find the the documentation of the  [tokenize package](https://www.nltk.org/api/nltk.tokenize.html) informative.\n",
        "\n",
        "#### Your task\n",
        "Tokenize the body field of a post using a Regular Expression and Treebank tokenizer (recall this as a standard tokenizer from Penn Treebank discussed in Lecture 1) and compare them:\n",
        "1.  Import the Regular Exp Tokenizer from NLTK.  \n",
        "2.  Create a regular expression tokenizer that that uses a simple pattern -- a sequence of one or more \"word characters\". (You might need to use a search engine to help identify the right pattern to do this.)\n",
        "3.  Create a tokenizer that tokenizes using the (Penn) Treebank Word tokenizer. Find the right tokenizer in the package documentation.  You may look at what other tokenizers are available.\n",
        "4. Tokenize the sample post below using each of the tokenizers and print the resulting tokens for each.\n",
        "5. Inspect and compare the output of the tokenizers.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-3UGSl8Ix_CG",
        "colab_type": "code",
        "outputId": "c04805ac-540d-4f64-91a3-b7f07fb23daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# A sample post to tokenize. \n",
        "\n",
        "# Get the body text of a post from the data frame.\n",
        "text = post_frame.iloc[10]['body']\n",
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I love cheese cake! I love both making and eating it, so I'm sad to see that they usually have the most crazy calorie counts, and I don't see many low cal recipes for them especially not ones that can work for many different people (for instance, I see a lot that require sugar free Jell-O, cool whip, Neufchatel cream cheese, very specific items that aren't available everywhere).\n",
            "\n",
            "[So here is a reciped I made!](http://imgur.com/a/z6VbS) it's very delicious, and I hope others will enjoy it too :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ck7Uv38BZwKK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2Dd8qIJZa-j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization matters\n",
        "Tokenization is a critical first choice in developing text applications.  Below are some questions to consider when comparing the tokenizers.\n",
        "\n",
        "- What are the key differences between the tokenizers?\n",
        "- How do they treat punctuation?\n",
        "- What happens to the link + URL?\n",
        "- What is a 'good' vs a 'bad' okenizer?  \n",
        "- How would you critically select one to use in an application? \n",
        "\n",
        "Are either of these tokenizers perfect? Consider how you would change the tokenizer to make it effective for the Reddit data. "
      ]
    },
    {
      "metadata": {
        "id": "uvqKgWz2afgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task\n",
        "Create  a simple *tokenize* function that uses the regular expression tokenizer.  The function takes a string and returns the output of the tokenizer. \n",
        "\n",
        "- Click SHOW CODE on the cell below to see the regular expression tokenizer (it's hidden because it's part of the answer to the previous task). Execute the cell so that you can use this tokenizer in your function.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RaW1TjfQapNq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u5ZU7Rdacj0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZGO-qm_F3SCC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(string):\n",
        "  return regexp_tokenizer.tokenize(string)\n",
        "\n",
        "tokenize(\"I love TaD and it's fun123-123.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSYr7NP7Vm8J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now use our tokenizer on all the posts on Reddit! \n",
        "- We use a powerful feature of Pandas called *apply* that executes a function that's passed as a parameter to every element of the series (similar to a Map operation).  \n",
        "- We pass in the tokenize function to apply. \n",
        "\n",
        "Execute the code below. Note: this may take a minute to operate on all posts.\n"
      ]
    },
    {
      "metadata": {
        "id": "M1MxcjK4VlR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# This tokenizes the body posts and creates vector of tokens for each post.\n",
        "# Note: This selections the body column from the posts only. \n",
        "all_posts_tokenized = post_frame.body.apply(tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcf9LbAhywIr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The result is a Pandas series that contains the tokens for each post. \n",
        "\n",
        "For analysis, let's flatten the tokens into a single value. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kX2UKTpRyp1D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "# A single variable with the (flattened) tokens from all posts.\n",
        "flat_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
        "\n",
        "print(flat_tokens[13:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7z3aWHkjWe5w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Token collection statistics"
      ]
    },
    {
      "metadata": {
        "id": "DJ9wa7K6dvpM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have all the tokens in the flat_tokens variable, let's use it to compute token statistics we talked about in lecture. "
      ]
    },
    {
      "metadata": {
        "id": "WlPkMj7DTI5e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task\n",
        "Write come code to print the following values:\n",
        "- What is the total number of tokens (N)?\n",
        "- What is the average length of a post in tokens? *Hint: *How do you find the total number of posts? "
      ]
    },
    {
      "metadata": {
        "id": "23li44bwd83W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJb269-E57Qv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- There should be roughly 4.8 million tokens. \n",
        "- Does this seem like a reasonable average length of a post?\n",
        "\n",
        "#### Optional task\n",
        "- On all_posts_tokenized compute the length of each post.  Compute statistics beyond the mean (min, max, stddev, median, etc...)\n"
      ]
    },
    {
      "metadata": {
        "id": "j0_d57cEejS6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's inspect the tokens in the collection."
      ]
    },
    {
      "metadata": {
        "id": "VL7XsanwD3er",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your Task\n",
        "\n",
        "- Print out the 50 most frequent (common) tokens in the reddit collection with their term frequencies (TF). \n",
        "  \n",
        "Use the python [collections.Counter](https://docs.python.org/2/library/collections.html) library. See it's documentation for examples on how to use it."
      ]
    },
    {
      "metadata": {
        "id": "rVRXT6U9e7vP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "moEaXn_gFNpl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Look at the other functions on the Counter object.  It's a very useful library for keeping track of counts. We'll be using it again in the future. "
      ]
    },
    {
      "metadata": {
        "id": "1ITIvzgu6dd5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2: Text Normalization\n",
        "\n",
        "In this section we will apply simple text normalization. We will write a function that takes raw tokens and normalizes them."
      ]
    },
    {
      "metadata": {
        "id": "SNev_-8J16nH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task\n",
        "\n",
        "Define a python function called *normalize* that:\n",
        "- Takes a sequence of *tokens* as input \n",
        "- Returns a list of *normalized tokens*\n",
        "- The function should perform the following normalization: lowercasing (basic String operation) and stem the tokens using the PorterStemmer (see also the [NLTK stem package](https://www.nltk.org/api/nltk.stem.html))."
      ]
    },
    {
      "metadata": {
        "id": "0ORFJKkGfeZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-HMHiXNAckU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Apply the normalize function to the flat tokens. \n",
        "This may take a 1-2 minutes to run over the entire collection (it is over almost 5 million tokens)."
      ]
    },
    {
      "metadata": {
        "id": "emCgRILUASPy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "normalized_tokens = normalize(flat_tokens)\n",
        "print(normalized_tokens[13:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5cZOfNIQ_vTn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Collect information on the vocabulary. "
      ]
    },
    {
      "metadata": {
        "id": "AxR8PyAN_7tT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Your task\n",
        "Fill in the blanks in the code below to compute the statistics for the normalized tokens. \n",
        "\n",
        "Tip: Python has built-in set() function to create distinctify the values."
      ]
    },
    {
      "metadata": {
        "id": "Tn37bEQAWZkY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set of unique tokens (from flat_tokens)\n",
        "B = \n",
        "\n",
        "# Set of unique normalized tokens --> the vocabulary\n",
        "V = \n",
        "\n",
        "# |N| - number of all tokens\n",
        "print (N)\n",
        "\n",
        "# |B|\n",
        "print( )\n",
        "\n",
        "# |V| \n",
        "print( )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVZBM7ZfBAQ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There should be approximately 140k unique raw tokens and about 86k normalized tokens. The vocabulary is significantly smaller than the number of unique raw tokens."
      ]
    },
    {
      "metadata": {
        "id": "zNMZ3XO_hpA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's inspect the most frequent normalized tokens.  Click SHOW CODE and execute the cell. "
      ]
    },
    {
      "metadata": {
        "id": "xxl-HBDtXM5g",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "normalized_word_count = collections.Counter(normalized_tokens)\n",
        "normalized_word_count.most_common(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmy1iZjvkhvE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations! We now have the basic vocabulary (V) as discussed in Lecture 1. "
      ]
    },
    {
      "metadata": {
        "id": "9Ui5S-3OKvEU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Stopwords **\n",
        "\n",
        "The most common words are functional words, often referred to as 'stop words' because they don't convey meaningful information for 'aboutness' as discussed in lecture.  In many applications we remove stopwords to remove 'noise', but in other cases they may be important to keep. You should be able to justify your decisions for when (and what) words are 'stop words'.\n",
        "\n",
        "\n",
        "** Application example **\n",
        "\n",
        "One example of where stopwords are very important is [automatic language identification](https://en.wikipedia.org/wiki/Language_identification). Language ID is a type of text classification task that we'll discuss later in the course.  In this case the words are important because stop words are not shared in common across languages, usually.  We can therefore identify text based on it's expected usage patterns of these words.  "
      ]
    },
    {
      "metadata": {
        "id": "AYKnsoyVYlbY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text processing summary\n",
        " At this point we've done the most important parts of text processing and normalization. We have the building blocks that we can build upon to represent text in the future. \n",
        "\n",
        "However, you could do more effective text processing by improving the tokenization and normalization functions. Consider a few possibilities:\n",
        " - What about URLs?\n",
        " - What about improved handling of contractions (e.g. what's)\n",
        " - What about removing Stopwords as part of normalization?\n",
        " - What about removing 'junk' tokens that are too long or too short to be meaningful?\n",
        " - What about pruning the vocabulary to remove rare tokens?\n"
      ]
    },
    {
      "metadata": {
        "id": "yXr9Q5uRit_5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "You've made good progress in this lab.  We have:\n",
        "- Introduced Colab\n",
        "- Learned basic libraries: Pandas, Counter, itertools, NLTK basic\n",
        "- Performed basic text processing: tokenization and normalization\n",
        "- Examined the structure of the Reddit dataset at the thread and post level\n",
        "- Computed collection statistics on the tokens, including creating a vocabulary\n",
        "\n",
        "In the next lab we will use these basics to represent text with one-hot encoding and bag-of-words representations and use these representations to look at post similarity and clustering. "
      ]
    },
    {
      "metadata": {
        "id": "UMltdfxuV0Dd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Finished\n",
        "Well done!\n",
        "\n",
        "Please can you complete the feedback form for this lab on Moodle: https://moodle.gla.ac.uk/mod/feedback/view.php?id=1104426\n",
        "\n"
      ]
    }
  ]
}